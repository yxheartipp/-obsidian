---
alias: []
Type: 
tags: ["#review"]
sr-due: 2024-04-17
sr-interval: 1
sr-ease: 250
---
### memtable数量过多
当memtable数量达到min-write-buffer-number-to-merge(默认值为1)

参数个时会触发flsush，Flush慢主要由于磁盘性能问题引起，当等待flush的memetable数量达到参数max-write-buffer-number时会完全停止写入。当max-write-buffer-number>3且等待flush的memetable数量>=参数max-write-buffer-number-1时会降低写入速度。

当由于memtable数量引起write stall时，内存充足的情况下可尝试调大max-write-buffer-number、max_background_jobs 、write_buffer_size 进行缓解。
### L0文件数量过多
当L0 sst文件数达到level0_slowdown_writes_trigger后会触发write stall 降低写入速度，当达到level0_stop_writes_trigger则完全停止写入。

当由于memtable数量引起write stall时，内存充足的情况下可尝试调大max_background_jobs 、write_buffer_size、min-write-buffer-number-to-merge进行缓解。
### 待compact数据量过多
当需要compact的文件数量达到soft_pending_compaction_byte参数值时会触发write stall，降低写入速度，当达到hard_pending_compaction_byte时会完全停止写入.

![[Pasted image 20240416141558.png]]
### 相关参数

**no_slowdown** ： writeOptions参数，如果no_slowdown不为0，那么不会sleep等待，会返回incomplete状态。用在一些延迟敏感的请求上。  
**max_background_flushes**: flush线程数，用于too many memtables场景  
**max_write_buffer_number**：同上  
**max_background_compactions**： LEVEL0 SST太多  
**write_buffer_size**： 改memtable大小  
**slowdown_trigger**: 用于too many sst0  
**stop_trigger**: 用于too many sst0  
"no_slowdown": write options中的变量，默认为false，即如果需要stall或者stop就进行等待。如果设置为true，当需要进行stop或者是stall时，返回status为incomplete。适用于不能容忍阻塞等待的写入。需要注意的是，对于设置了no_slowdown的写请求，不能跟没有设置no_slowdown的请求进行joinbatchgroup。

减慢/停止的触发和等待compaction的字节数限制是每个column family单独配置的，但是write stall 是应用到整个数据库的，这意味着如果一个column family触发write stall，整个数据库都会被stall。

**subcompact**：线程的数量，是受background线程数量
**dynamic_level**：会导致频繁的触发compaction

有无重复数据

当写入 memtable 的数据大小达到 write_buffer_size 参数设置值，memtable 会转变成 immutable 等待 flush 到 L0 层。如果 memtable 已经达到 write_buffer_size 大小，内存中 memtable 数据量达到了 max_write_buffer_number 限制，那么会触发 RocksDB 的 write stall ，等待 immutable flush 到 L0 层。

这种由于 memtable 太多导致 stall 的情况一般是因为瞬间写入量比较大，memtable flush 到磁盘比较慢导致的。如果磁盘写入速度不能改善，并且只有业务高峰值会出现这种情况，可以通过调大对应 cf 的 max_write_buffer_number 参数来缓解。

write_buffer_size 控制一个写内存 memtable 的大小，当这个 memtable 写满之后数据会被固化到磁盘上，这个值越大批量写入的性能越好。max_write_buffer_number 控制写内存 memtable 数目数量。但是这两个值不是越大越好，太大会延迟一个 DB 被重新打开时的数据加载时间。

在这一步骤可能会有 memtable 文件过多导致 write stall，影响写入速度。

只要持续性高强度写入就会触发导致触发写缓慢，如何解决。

max_bytes_for_level_base

estimated_compaction_bytes

现在，我们得到了X轴时间对齐的多个变量，如下图，estimated_compaction_bytes的剧烈变化(增大），导致了QPS骤降为0:

进一步，我们把L0 size与QPS关联起来：如下图，每当巨大的L0 size被compact下去(case 1)，size为0时，我们也遇到了write stall。


等待 compaction 的大小。Compaction pending bytes 太多会导致 stall,，当 Compaction pending bytes 达到 rocksdb.defaultcf.soft-pending-compaction-bytes-limit 参数值（默认是 64G）之后，RocksDB 会放慢写入速度；如果 Compaction pending bytes 达到 rocksdb.defaultcf.hard-pending-compaction-bytes-limit 参数值（默认是 256G）之后，RocksDB 会停止写入。

如果是 Compaction pending bytes 过多导致 write stall 有几种解决方案：

- 调大 soft-pending-compaction-bytes-limit 和 hard-pending-compaction-bytes-limit 参数，防止触发 write stall ，但是这个只是治标不治本，根本原因应该是 compaction 慢
- 确认用户是否有做过 compaction 限流，如果有，那需要放开一点限制看看
- 如果磁盘 IO 能力持续跟不上，建议扩容。
- 如果磁盘的吞吐达到了上限导致 write stall ，但是 CPU 资源比较充足，可以尝试采用压缩率更高的压缩算法来缓解磁盘压力，使用 CPU 资源换磁盘资源。比如 default cf compaction 压力比较大，调整参数 [rocksdb.defaultcf] compression-per-level = [“no”, “no”, “lz4”, “lz4”, “lz4”, “zstd”, “zstd”] 改成 compression-per-level = [“no”, “no”, “zstd”, “zstd”, “zstd”, “zstd”, “zstd”]
- TiKV-Details → RocksDB KV/RocksDB raft → Compression ratio
### bluefs 参数
1. write buffer
2. compact_log_size